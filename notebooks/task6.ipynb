{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations for Task 6\n",
    "\n",
    "In previous task I have tested different classifiers provided in the sklearn toolkit.\n",
    "After running the main script following summary is displayed:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "================================\n",
    "A. Age and Fare not categorized:\n",
    "================================\n",
    "QuadraticDiscriminantAnalysis 0.8208955223880597\n",
    "LogisticRegressionCV 0.8059701492537313\n",
    "ExtraTreesClassifier 0.7910447761194029\n",
    "RandomForestClassifier 0.7910447761194029\n",
    "GradientBoostingClassifier 0.7910447761194029\n",
    "LogisticRegression 0.7761194029850746\n",
    "RidgeClassifierCV 0.7611940298507462\n",
    "AdaBoostClassifier 0.7611940298507462\n",
    "LinearDiscriminantAnalysis 0.7611940298507462\n",
    "RidgeClassifier 0.7611940298507462\n",
    "ExtraTreeClassifier 0.746268656716418\n",
    "CalibratedClassifierCV 0.746268656716418\n",
    "LinearSVC 0.746268656716418\n",
    "DecisionTreeClassifier 0.7313432835820896\n",
    "BaggingClassifier 0.6865671641791045\n",
    "BernoulliNB 0.6865671641791045\n",
    "GaussianNB 0.6865671641791045\n",
    "GaussianProcessClassifier 0.6567164179104478\n",
    "LabelSpreading 0.6417910447761194\n",
    "LabelPropagation 0.6417910447761194\n",
    "PassiveAggressiveClassifier 0.6268656716417911\n",
    "NuSVC 0.6119402985074627\n",
    "SVC 0.5970149253731343\n",
    "KNeighborsClassifier 0.5970149253731343\n",
    "Perceptron 0.582089552238806\n",
    "SGDClassifier 0.5671641791044776\n",
    "MultinomialNB 0.5373134328358209\n",
    "MLPClassifier 0.5223880597014925\n",
    "NearestCentroid 0.5074626865671642\n",
    "\n",
    "============================\n",
    "B. Age and Fare categorized:\n",
    "============================\n",
    "SVC 0.8059701492537313\n",
    "NuSVC 0.8059701492537313\n",
    "MLPClassifier 0.7910447761194029\n",
    "LabelSpreading 0.7910447761194029\n",
    "ExtraTreesClassifier 0.7761194029850746\n",
    "RandomForestClassifier 0.7761194029850746\n",
    "LogisticRegression 0.7761194029850746\n",
    "LabelPropagation 0.7761194029850746\n",
    "RidgeClassifierCV 0.7611940298507462\n",
    "LinearDiscriminantAnalysis 0.7611940298507462\n",
    "CalibratedClassifierCV 0.7611940298507462\n",
    "LinearSVC 0.7611940298507462\n",
    "GaussianProcessClassifier 0.7611940298507462\n",
    "RidgeClassifier 0.7611940298507462\n",
    "GradientBoostingClassifier 0.7611940298507462\n",
    "AdaBoostClassifier 0.746268656716418\n",
    "LogisticRegressionCV 0.746268656716418\n",
    "PassiveAggressiveClassifier 0.7313432835820896\n",
    "DecisionTreeClassifier 0.7313432835820896\n",
    "BaggingClassifier 0.7164179104477612\n",
    "ExtraTreeClassifier 0.7164179104477612\n",
    "BernoulliNB 0.7014925373134329\n",
    "GaussianNB 0.7014925373134329\n",
    "SGDClassifier 0.6865671641791045\n",
    "KNeighborsClassifier 0.6716417910447762\n",
    "QuadraticDiscriminantAnalysis 0.6268656716417911\n",
    "Perceptron 0.6268656716417911\n",
    "MultinomialNB 0.5970149253731343\n",
    "NearestCentroid 0.5223880597014925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First column is the name of the classifier, while the second is the accuracy of prediction. In order to calculate the accuracy, function accuracy_score from the sklearn.metrics module was used. However, there is plenty of different scoring functions in the sklearn.metrics module (https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) and each of them (depending on needs and preferences) may be used for comparing different models.\n",
    "\n",
    "Both lists contain results for all available classifiers, obtained for slightly different sets of features. In the first case (A) there are original features provided in the initial commit (only the PassengerId was removed in comparison to the original initial preprocessing). In the second case (B), two features (Age and Fare) were categorized (histogrammed) into few categories.\n",
    "\n",
    "For A case, the QuadraticDiscriminantAnalysis has the highest accuracy. For B case, the SVC has the highest accuracy. In both cases, chosen models provide higher accuracy than the random forest.\n",
    "\n",
    "If I would have to choose the classifier without checking all available solutions, I believe I would choose the classifier based on the support vector machine (SVC) as this is typical ML model for classification problem (https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). If the data set would be higher I would probably use the SGD classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
